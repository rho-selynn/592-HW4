{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "592-HW4",
      "provenance": [],
      "authorship_tag": "ABX9TyP3grvUficDnMnkf/3uoe8O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rho-selynn/592-HW4/blob/main/592_HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUUUGVw849jI"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "# This mouunts your google drive to the current runtime\n",
        "drive.mount('/content/mnt')\n",
        "# We define a notebook path\n",
        "nb_path = '/content/notebooks'\n",
        "# We create a symbolic link from our drive's default \"Colab Notebooks\" folder to nb_path\n",
        "os.symlink('/content/mnt/My Drive/Colab Notebooks', nb_path)\n",
        "# Insert nb path\n",
        "sys.path.insert(0, nb_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repos from the assignment pdf\n",
        "\n",
        "OpenAI’s gym repository https://github.com/openai/gym\n",
        "\n",
        "PFRL’s repository https://github.com/pfnet/pfrl\n",
        "\n",
        "PFRL Quickstart Guide: https://github.com/pfnet/pfrl/blob/master/examples/quickstart/quickstart.ipynb"
      ],
      "metadata": {
        "id": "Z8MHALxbjjAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym\n",
        "pip install pfrl"
      ],
      "metadata": {
        "id": "356HVZRG5mU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going through PFRL Quickstart Guide"
      ],
      "metadata": {
        "id": "rMR6wFBIj-Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pfrl\n",
        "import torch\n",
        "import torch.nn\n",
        "import gym\n",
        "import numpy"
      ],
      "metadata": {
        "id": "jo-8yo5xjLse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining our environment\n",
        "env = gym.make('CartPole-v0')\n",
        "print('observation space:', env.observation_space)\n",
        "print('action space:', env.action_space)\n",
        "\n",
        "obs = env.reset()\n",
        "print('initial observation:', obs)\n",
        "\n",
        "action = env.action_space.sample()\n",
        "obs, r, done, info = env.step(action)\n",
        "print('next observation:', obs)\n",
        "print('reward:', r)\n",
        "print('done:', done)\n",
        "print('info:', info)\n",
        "\n",
        "# Uncomment to open a GUI window rendering the current state of the environment\n",
        "# env.render()"
      ],
      "metadata": {
        "id": "16M6hLzjjq5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QFunction(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, obs_size, n_actions):\n",
        "        super().__init__()\n",
        "        self.l1 = torch.nn.Linear(obs_size, 50)\n",
        "        self.l2 = torch.nn.Linear(50, 50)\n",
        "        self.l3 = torch.nn.Linear(50, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        h = torch.nn.functional.relu(self.l1(h))\n",
        "        h = torch.nn.functional.relu(self.l2(h))\n",
        "        h = self.l3(h)\n",
        "        return pfrl.action_value.DiscreteActionValue(h)\n",
        "\n",
        "obs_size = env.observation_space.low.size\n",
        "n_actions = env.action_space.n\n",
        "q_func = QFunction(obs_size, n_actions)"
      ],
      "metadata": {
        "id": "rB9XdA0zkmHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_func2 = torch.nn.Sequential(\n",
        "    torch.nn.Linear(obs_size, 50),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(50, 50),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(50, n_actions),\n",
        "    pfrl.q_functions.DiscreteActionValueHead(),\n",
        ")"
      ],
      "metadata": {
        "id": "nQGcqyOLkyVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
        "optimizer = torch.optim.Adam(q_func.parameters(), eps=1e-2)"
      ],
      "metadata": {
        "id": "08KOFpMpkz8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is to create an agent and an environment\n",
        "\n",
        "# Set the discount factor that discounts future rewards.\n",
        "gamma = 0.9\n",
        "\n",
        "# Use epsilon-greedy for exploration\n",
        "explorer = pfrl.explorers.ConstantEpsilonGreedy(\n",
        "    epsilon=0.3, random_action_func=env.action_space.sample)\n",
        "\n",
        "# DQN uses Experience Replay.\n",
        "# Specify a replay buffer and its capacity.\n",
        "replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=10 ** 6)\n",
        "\n",
        "# Since observations from CartPole-v0 is numpy.float64 while\n",
        "# As PyTorch only accepts numpy.float32 by default, specify\n",
        "# a converter as a feature extractor function phi.\n",
        "phi = lambda x: x.astype(numpy.float32, copy=False)\n",
        "\n",
        "# Set the device id to use GPU. To use CPU only, set it to -1.\n",
        "gpu = -1\n",
        "\n",
        "# Now create an agent that will interact with the environment.\n",
        "agent = pfrl.agents.DoubleDQN(\n",
        "    q_func,\n",
        "    optimizer,\n",
        "    replay_buffer,\n",
        "    gamma,\n",
        "    explorer,\n",
        "    replay_start_size=500,\n",
        "    update_interval=1,\n",
        "    target_update_interval=100,\n",
        "    phi=phi,\n",
        "    gpu=gpu,\n",
        ")"
      ],
      "metadata": {
        "id": "YkqBlYGKk1kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 300\n",
        "max_episode_len = 200\n",
        "for i in range(1, n_episodes + 1):\n",
        "    obs = env.reset()\n",
        "    R = 0  # return (sum of rewards)\n",
        "    t = 0  # time step\n",
        "    while True:\n",
        "        # Uncomment to watch the behavior in a GUI window\n",
        "        # env.render()\n",
        "        action = agent.act(obs)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        R += reward\n",
        "        t += 1\n",
        "        reset = t == max_episode_len\n",
        "        agent.observe(obs, reward, done, reset)\n",
        "        if done or reset:\n",
        "            break\n",
        "    if i % 10 == 0:\n",
        "        print('episode:', i, 'R:', R)\n",
        "    if i % 50 == 0:\n",
        "        print('statistics:', agent.get_statistics())\n",
        "print('Finished.')"
      ],
      "metadata": {
        "id": "qd6BbKu-k-Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CartPole-v0's max achievable return is 200\n",
        "with agent.eval_mode():\n",
        "    for i in range(10):\n",
        "        obs = env.reset()\n",
        "        R = 0\n",
        "        t = 0\n",
        "        while True:\n",
        "            # Uncomment to watch the behavior in a GUI window\n",
        "            # env.render()\n",
        "            action = agent.act(obs)\n",
        "            obs, r, done, _ = env.step(action)\n",
        "            R += r\n",
        "            t += 1\n",
        "            reset = t == 200\n",
        "            agent.observe(obs, r, done, reset)\n",
        "            if done or reset:\n",
        "                break\n",
        "        print('evaluation episode:', i, 'R:', R)"
      ],
      "metadata": {
        "id": "9FYiPWo0lAn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save an agent to the 'agent' directory\n",
        "agent.save('agent')\n",
        "\n",
        "# Uncomment to load an agent from the 'agent' directory\n",
        "# agent.load('agent')"
      ],
      "metadata": {
        "id": "mPmfqPQYlGSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's some nify utility functions that PFRL has to make our lives easier\n",
        "# Set up the logger to print info messages for understandability.\n",
        "import logging\n",
        "import sys\n",
        "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
        "\n",
        "pfrl.experiments.train_agent_with_evaluation(\n",
        "    agent,\n",
        "    env,\n",
        "    steps=2000,           # Train the agent for 2000 steps\n",
        "    eval_n_steps=None,       # We evaluate for episodes, not time\n",
        "    eval_n_episodes=10,       # 10 episodes are sampled for each evaluation\n",
        "    train_max_episode_len=200,  # Maximum length of each episode\n",
        "    eval_interval=1000,   # Evaluate the agent after every 1000 steps\n",
        "    outdir='result',      # Save everything to 'result' directory\n",
        ")"
      ],
      "metadata": {
        "id": "6ps76XBAlO6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}